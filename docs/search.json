[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi",
    "section": "",
    "text": "I am Sam. üòä\n\nData Science & Analytics / Plant Lover / Reader\n\nGlad you are here! This page is to showcase my skills and passions.\nUltimately, I hope to inspire you to pursue what you love, aka being in the flow!\n‚Äú\nThe flow state, often called ‚Äúbeing in the zone,‚Äù  is a mental state where you are fully absorbed in an activity, and losing track of time.\n‚Äú\n\n\n\nArea of Interest"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Sam P",
    "section": "",
    "text": "See my latest projects:"
  },
  {
    "objectID": "analysis.html#lets-get-started",
    "href": "analysis.html#lets-get-started",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Let‚Äôs get started!",
    "text": "Let‚Äôs get started!\nThis analysis is performed using Python in a Jupyter Notebook environment. Ensure you have the following libraries installed:\n\npandas\ndatetime\nnumpy\nrequests\nmatplotlib\nbeautifulsoup4\nseaborn\nmatplotlib\nsklearn\n\n\nExtracting the Data via Web Scraping\nData is scrapped from the U.S. Department of the Treasury. Date range: 1990 to Present\nUse the first URL to inspect the table structure, and get the table headings, then loop through all the URLs to get the table data.\n\n\nCode\nfrom bs4 import BeautifulSoup as BS\nimport requests as req\nimport datetime\nimport pandas as pd\nimport numpy as np\n\nurls=[]\nurl = \"https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=xxxx\"\n\nstart_year = 1990\nend_year = datetime.date.today().year\n\nyears = [str(yr) for yr in range(start_year, end_year+1)]\n\nfor year in years:\n    urls.append(url.replace('xxxx', year))\n\n# inspect the first URL to get the table headings\nhtml_text = req.get(urls[0]).text\nsoup=BS(html_text, 'html.parser')\n\nprint('Scrapping from website:', soup.title.text)\nprint(f'This script is to scrap US Treasury Rate data from {start_year} to {end_year}.')\n\n# column headings\nheadings = []\n# table = soup.find('table', class_ = 'usa-table views-table views-view-table cols-26') \n# table_rows = table.find_all('tr')\n\nfor item in soup.find_all('th'):\n    headings.append(item.text)\n# print('Table headings &lt;th&gt;:', headings)\n\n# now scrap all the table data/ td from all the URLs\nall_rows = []\nfor url in urls:\n    html_text = req.get(url).text\n    soup = BS(html_text, 'html.parser')\n    table_rows = soup.find_all('tr')               \n    for tr in table_rows[1:]:\n        td = tr.find_all('td')\n        row=[i.text.strip('\\n') for i in td]       #i is each td element, i.text is to find all text in that i, .strip to remove entries starts with \\n\n        all_rows.append(row)\ndf=pd.DataFrame(data=all_rows, columns=headings)\n\n# df.to_csv('raw_US_Treasury_Rates.csv', index=False)\n\n\nScrapping from website: Resource Center | U.S. Department of the Treasury\nThis script is to scrap US Treasury Rate data from 1990 to 2025.\n\n\n\n\nReviewing and Cleaning the Data\nInspecting dataframe via df.describe(), df.info() and df.head(). There are a few things to clean up:\n\nRemove leading and trailing spaces\nReplace all ‚ÄòN/A‚Äô with NaN\nDrop columns with all NaN values\nconvert the ‚ÄòDate‚Äô column to datetime format\nconvert the rest of the columns to float\n\n\nAll done! Ready for analysis.\n\n\nCode\n# display(df.describe(), df.head())\n# noticed there are space in N/A cell, \n# print(\"N/A cell: '\", df['20 YR'][0], \"'\")\n# remove all blank spaces in the dataframe, except headers\nclean_df = df.iloc[:,:].map(lambda x:x.strip())\n# converting to lower case, if cell equals to 'n/a', then replace with empty string\nclean_df = clean_df.map(lambda x:np.nan if (x.lower() == 'n/a' or x =='') else x)\n# dropping all columns with all NaN values\nclean_df.dropna(axis=1, how='all', inplace=True)\n# change Date column to datetime and the rest to float\nclean_df[clean_df.columns[0]] = pd.to_datetime(clean_df[clean_df.columns[0]])\nfor col in clean_df.columns[1:]:\n    clean_df[col] = clean_df[col].astype(str).astype(float)\n# clean_df.info()\n# clean_df.head()"
  },
  {
    "objectID": "analysis.html#initial-observations",
    "href": "analysis.html#initial-observations",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Initial observations",
    "text": "Initial observations\n\nI choose you, Seaborn and Matplotlib!\n\n\nData Distribution\nUtilizing seaborn and matplotlib, we will visualize the trends and distribution in the U.S. Treasury Par Yield Curve Rates over time.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Melt df to long format\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\n\nf, ax = plt.subplots(figsize=(7, 6))\n# Plot the orbital period with horizontal boxes, vlag - blue/pink diverging color palette\nsns.boxplot(\n    data, x=\"Value\", y=\"Series\", hue=\"Series\", palette=\"vlag\"\n)\n\nax.set_xlabel(\"US Treasury Rates (%)\")\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\n\n\n\n\n\n\n\n\n\nBox plots allows us to view the distribution of rates for each maturity period. We can also observe that the short term rates has generally a lower rate than the long term rates, which is expected.\n1.5Mo and 4Mo maturity rate have significantly smaller range of data. Upon review, it appears that the 1.5Mo and 4Mo maturity rates were only added to the dataset in recent years, which has less data points compared to other rates.\n\n\nTrends Over Time\n\n\nCode\nsns.set_theme(style=\"whitegrid\")\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='crest')\nplt.title(\"Data Overview\")\nplt.ylabel(\"US Treasury Rates (%)\")\n# plt.\nplt.show()\n\n\n\n\n\n\n\n\n\nThe line chart below shows the trends of different maturity rates from 1990 to 2024 are generally trending down. Unexpected upward spikes starting around 2021, likely due to economic recovery post-pandemic.\nMoreover, the short-term rates (1 month, 3 month, 6 month, 1 year) are more volatile, while the long-term rates (10 year, 20 year, 30 year) are more stable. There are a few period that the short-term rates spike above the long-term rates, indicating an inverted yield curve, which we will explore this further."
  },
  {
    "objectID": "analysis.html#inverted-yield-curve",
    "href": "analysis.html#inverted-yield-curve",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Inverted Yield Curve",
    "text": "Inverted Yield Curve\n‚ÄúWhen short-term rates are higher than long-term rates.‚Äù\nEconomists use the yield curve, which compares the interest rates difference bweteen ‚Äú10-year and 3-month‚Äù or ‚Äú10-year and 2-year‚Äù treasury rate, to determine if short-term investments are more profitable than long-term ones. When the yield curve inverts and the difference drops below zero, it serves as a warning sign of an impending recession.\n\n\nCode\n#calculate the inversed rate\nclean_df['10Yr_3Mo'] = clean_df['10 Yr']-clean_df['3 Mo']\nclean_df['10Yr_2Yr'] = clean_df['10 Yr']-clean_df['2 Yr']\n\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\ndata = data[data['Series'].isin(['10Yr_3Mo', '10Yr_2Yr'])]\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='flare')\nplt.title(\"10yr vs 2 Years/ 3 Months' Maturity Rate\")\nplt.ylabel(\"Inversed Rate (%)\")\nplt.legend(loc=\"upper right\")\nplt.axhline(y=0, color='grey',linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe assumption of recession is supported by the last 30 years of invasion occurrences. Recession happened in early 2000 (dot-com bubble), 2007-2009 (housing bubble), and 2020 (pendemic). The 3 recessions mentioned experienced a decrease below 0, as shown in the chart above."
  },
  {
    "objectID": "analysis.html#principal-component-analysis-pca",
    "href": "analysis.html#principal-component-analysis-pca",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nSklearn, Here we go!\n\n\nStandardize df and Covariance Matrix Heatmap\nPCA allows us to reduce the dimensionality of our dataset while retaining most of the variance. This is particularly useful when dealing with datasets that have many correlated variables, as it helps to identify the underlying structure and patterns in the data.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler \nimport numpy.linalg as LA\n\ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\ncolnames=list(data.columns)\n# standardize df\nsc=StandardScaler()\ndata1=sc.fit_transform(data)\ndata1=pd.DataFrame(data1)\n\ndata1.columns = colnames\n\n# covariance matrix\ncov=pd.DataFrame.cov(data1)\n\nplt.figure(figsize=(12,8))\nsns.heatmap(cov, cmap=\"Spectral\", center=0, annot=True, fmt=\".2f\")\n\nplt.title(\"Covariance Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\nViewing the covariance matrix, we can see that the short-term rates (1Mo, 3Mo, 6Mo, 1Yr) are highly correlated with each other, as are the long-term rates (10Yr, 20Yr, 30Yr). However, there is a lower correlation between short-term and long-term rates, which is expected given their different sensitivities to economic conditions.\n\n\nCode\neval, evec = LA.eig(cov)\nfor i in range(3):\n    print('The PC{} is {}, its variance is {}, % variance is {}%, % cumulative variance is {}% '.format(\n        i+1\n        , np.round(evec[:, i],2)\n        , round(eval[i],2)\n        , round(eval[i]/eval.sum()*100,2)\n        , round(eval[:i+1].sum()/eval.sum()*100,2)))\n    \ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\n# bar graph\nPCs = [f'PC{i+1}' for i in range(11)]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nEigenValues = eval\nax.bar(PCs,EigenValues)\nplt.show()    \n\n\nThe PC1 is [0.3  0.3  0.31 0.31 0.31 0.32 0.32 0.31 0.3  0.27 0.26], its variance is 9.61, % variance is 87.36%, % cumulative variance is 87.36% \nThe PC2 is [-0.28 -0.29 -0.28 -0.26 -0.2  -0.12  0.06  0.19  0.33  0.46  0.52], its variance is 1.25, % variance is 11.35%, % cumulative variance is 98.71% \nThe PC3 is [ 0.44  0.33  0.19  0.   -0.25 -0.38 -0.41 -0.31 -0.07  0.19  0.38], its variance is 0.12, % variance is 1.06%, % cumulative variance is 99.77% \n\n\n\n\n\n\n\n\n\nYou can see that PC1 and PC2 explain most of the variance in the data (98.7% combined). The majority of the information in the original dataset can be captured in just two dimensions, which is a significant reduction from the original 11 dimensions.\nPC1 has coefficients that are all positive and relatively similar in magnitude (~0.3). It represents a general trend across all maturity rates. In contrast, First 6 of the PC2 coefficients are negative, and the last 5 are positive. This suggests PC2 captures the contrast between short-term and long-term rates.\n\n\nPCA Biplot\nTo further analyze PC1 and PC2, we will review are 2 main components in the biplot: arrows and points. The arrows represent the original variables (maturities), while the points represent the observations (dates).\n\n\nCode\nPCs = np.dot(data1, evec[:, :2])  # matrix multiplication: 11 variables (maturities) * weights (PC1, PC2)\n\ndata1 = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata1.dropna(inplace=True)\ndata1 = data1[['Date']]\ndata1['Year'] = data1['Date'].dt.year\ndata1.reset_index(drop=True, inplace=True)\npc_df = pd.DataFrame(PCs, columns=['PC1', 'PC2'])\npc_df = pd.concat([data1, pc_df], axis = 1)\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data = pc_df, x='PC1', y='PC2', alpha=0.7, hue = 'Year', palette='coolwarm')\n\n# Add arrows for variable loadings\nfor i, var in enumerate(data.columns):\n    #arrow start from 0,0, expand at the direction of PC1 and 2 for each maturity, *5 to make the arrows visible \n    plt.arrow(0, 0, evec[i,0]*5, evec[i,1]*5, color='r', alpha=0.7, head_width=0.05)\n    plt.text(evec[i,0]*5.2, evec[i,1]*5.2, var, color='r', ha='center', va='center')\n\n# 87.3% data explained by PC1 and 11.4% explained by PC2\nplt.xlabel(\"PC1 (%.1f%%)\" % (eval[0]/eval.sum()*100))\nplt.ylabel(\"PC2 (%.1f%%)\" % (eval[1]/eval.sum()*100))\n# horizontal and vertical lines at 0, y and x\nplt.axhline(0, color='grey', linewidth=1)\nplt.axvline(0, color='grey', linewidth=1)\nplt.title(\"PCA Biplot\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoints and Clusters\nEach point is weighted value of PC1 and PC2 for each observation (date). The position of the points indicates how the observations relate to the PC. The points are colored by year with a gradient from blue (earlier years) to red (later years). Each year has a distinct cluster, and the clusters allow us to observe how the observations change over time in relation to the PC.\n\n\nArrows (Loadings)\nThe arrows (loadings) represents the original variables (maturities). The arrows are pinned at the origin (0,0). The direction represents how each variable contributes to the PC, while the length indicates the strength of the contribution.\n\nLength of Arrows\nThe length of the arrows provides insight into the importance of each variable in explaining the variance captured by the PC. 30Yr has the longest arrow, indicating that the strongest influence on the PC1. In contrast, 5Yr has a shorter arrow, suggesting that lesser contribution to the variance.\n\n\nAngles of Arrows\nThe arrows for 1Mo, 3Mo, 6Mo, and 1Yr are close together, indicating that these short-term rates are highly correlated, same as arrows of 10Yr, 20Yr, and 30Yr for long-term rates. The arrows for 1Mo and 30Yr point in nearly opposite directions, suggesting a negative correlation between short-term and long-term rates. The arrows for 10Yr and 3Yr are nearly perpendicular, which indicates that the variables are uncorrelated.\n\n\nDirection of Arrows\n\n20Yr and 30Yr have high positive loadings on PC1 as they are closer to top axis/ loadings of PC1. 1Mo and 3Mo have lower loadings on PC1 smaller impact on the overall level of interest rates. None of the maturities have high loadings on PC2, indicating that PC2 captures the contrast between short-term and long-term rates rather than being dominated by any single maturity."
  },
  {
    "objectID": "analysis.html#so-whats-the-takeaway",
    "href": "analysis.html#so-whats-the-takeaway",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "So, What‚Äôs the takeaway?",
    "text": "So, What‚Äôs the takeaway?\nThe U.S. Treasury Par Yield Curve Rates have shown a general downward trend over the past few decades, with notable volatility in short-term rates. The occurrence of inverted yield curves has historically been a precursor to recessions, highlighting the importance of monitoring these rates for economic forecasting. As of October 2025, we are seeing yield curve returning to normal after the recent inversion in 2024, indicating potential economic recovery.\nWith covariance matrix heatmap and PCA analysis, we can see the strong correlations among short-term rates and among long-term rates, as well as the distinct patterns in how these rates have evolved over time. The PCA biplot provides a clear visualization of that relationships, emphasizing the contrast between short-term and long-term rates and their respective contributions to the overall variance in the dataset. The main source of variation in the yield curve comes from changes in long-term interest rates. In other words, the overall movement in the data is largely driven by the behavior of 20Yr and 30Yr bonds. In other words, the long bonds set the tone: when they move, the whole curve follows."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Let‚Äôs Connect",
    "section": "",
    "text": "You can find me here:\n\nüíº LinkedIn\nüßë‚Äçüíª GitHub \n‚úâÔ∏è Contact: theflow365@emailhub.kr"
  },
  {
    "objectID": "analysis.html#preview",
    "href": "analysis.html#preview",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "",
    "text": "collecting data via beautiful soul (web scraper)\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure / Distribution\nLearn about PCA and yield dynamics"
  },
  {
    "objectID": "webScraper.html",
    "href": "webScraper.html",
    "title": "US Treasury Bond Web Scrapper",
    "section": "",
    "text": "collecting data via Beautiful Soup (web scraper)\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure & Distribution"
  },
  {
    "objectID": "webScraper.html#preview",
    "href": "webScraper.html#preview",
    "title": "US Treasury Bond Web Scrapper",
    "section": "",
    "text": "collecting data via Beautiful Soup (web scraper)\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure & Distribution"
  },
  {
    "objectID": "webScraper.html#lets-get-started",
    "href": "webScraper.html#lets-get-started",
    "title": "US Treasury Bond Web Scrapper",
    "section": "Let‚Äôs Get Started!",
    "text": "Let‚Äôs Get Started!\nThis analysis is performed using Python in a Jupyter Notebook environment. Make sure you have the following libraries installed: * pandas * datetime * numpy * requests * matplotlib * beautifulsoup4 * seaborn\n\nReview URL\nRecently, I stumbled upon a government website that lists daily Treasury rates by year and maturity. I noticed that the data could be accessed simply by changing the last four digits of the URL, so I built a scraper to pull every year‚Äôs data from 1990 all the way to today.\nTry it yourself, change ‚Äúxxxx‚Äù to the desired year: https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=xxxx\n\n\nInspecting HTML\nUse the latest URL (current year) to inspect the HTML table structure: headings and rows. You‚Äôll notice there are 26 headings. The heading text looks clean, so we can store them to generate a DataFrame later. Within the table rows, there‚Äôs quite a bit of blank space in each cell, which is represented as text ‚ÄòN/A‚Äô. We‚Äôll need to clean that up shortly.\n\n\nCode\nfrom bs4 import BeautifulSoup as BS\nimport requests as req\nimport datetime\nimport pandas as pd\nimport numpy as np\n\nurls=[]\nurl = \"https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=xxxx\"\n\nstart_year = 1990\nend_year = datetime.date.today().year\n\nyears = [str(yr) for yr in range(start_year, end_year+1)]\n\nfor year in years:\n    urls.append(url.replace('xxxx', year))\n\n# inspect the last URL to get the table headings\nhtml_text = req.get(urls[-1]).text\nsoup=BS(html_text, 'html.parser')\n\nprint('Scrapping from website:', soup.title.text)\nprint(f'This script is to scrape US Treasury Rate data from {start_year} to {end_year}.')\n\n# table = soup.find_all('tr') \n\n# column headings\nheadings = []\nfor item in soup.find_all('th'):\n    headings.append(item.text)\nprint('Table headings &lt;th&gt;:', headings)\nprint('Number of headings:', len(headings))\n\n# data rows\nall_rows = []\nfor item in soup.find_all('td'):\n    all_rows.append(item.text)\nprint('Table data &lt;td&gt;:', all_rows[:26])\n\n\nScrapping from website: Resource Center | U.S. Department of the Treasury\nThis script is to scrape US Treasury Rate data from 1990 to 2025.\nTable headings &lt;th&gt;: ['Date', '20 YR', '30 YR', 'Extrapolation Factor', '6 WEEKS BANK DISCOUNT', 'COUPON EQUIVALENT', '8 WEEKS BANK DISCOUNT', 'COUPON EQUIVALENT', '17 WEEKS BANK DISCOUNT', 'COUPON EQUIVALENT', '52 WEEKS BANK DISCOUNT', ' COUPON EQUIVALENT', '1 Mo', '1.5 Mo', '2 Mo', '3 Mo', '4 Mo', '6 Mo', '1 Yr', '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']\nNumber of headings: 26\nTable data &lt;td&gt;: ['01/02/2025\\n', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', '4.45          ', 'N/A          ', '4.36          ', '4.36          ', '4.31          ', '4.25          ', '4.17          ', '4.25          ', '4.29          ', '4.38          ', '4.47          ', '4.57          ', '4.86          ', '4.79          ']\n\n\nAfter reviewing the table structure, we can iterate through all the URLs to extract the complete dataset. All data is stored in a dataframe and exported as a CSV file for further analysis.\n\n\nCode\n# now scrape all the table data/ td from all the URLs\nall_rows = []\nfor url in urls:\n    html_text = req.get(url).text\n    soup = BS(html_text, 'html.parser')\n    table_rows = soup.find_all('tr')               \n    for tr in table_rows[1:]:\n        td = tr.find_all('td')\n        row=[i.text.strip('\\n') for i in td]       #i is each td element, i.text is to find all text in that i, .strip to remove entries starts with \\n\n        all_rows.append(row)\ndf=pd.DataFrame(data=all_rows, columns=headings)\ndf.to_csv('raw_US_Treasury_Rates.csv', index=False)\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\n20 YR\n30 YR\nExtrapolation Factor\n6 WEEKS BANK DISCOUNT\nCOUPON EQUIVALENT\n8 WEEKS BANK DISCOUNT\nCOUPON EQUIVALENT\n17 WEEKS BANK DISCOUNT\nCOUPON EQUIVALENT\n...\n4 Mo\n6 Mo\n1 Yr\n2 Yr\n3 Yr\n5 Yr\n7 Yr\n10 Yr\n20 Yr\n30 Yr\n\n\n\n\n0\n01/02/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.89\n7.81\n7.87\n7.90\n7.87\n7.98\n7.94\nN/A\n8.00\n\n\n1\n01/03/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.94\n7.85\n7.94\n7.96\n7.92\n8.04\n7.99\nN/A\n8.04\n\n\n2\n01/04/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.90\n7.82\n7.92\n7.93\n7.91\n8.02\n7.98\nN/A\n8.04\n\n\n3\n01/05/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.85\n7.79\n7.90\n7.94\n7.92\n8.03\n7.99\nN/A\n8.06\n\n\n4\n01/08/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.88\n7.81\n7.90\n7.95\n7.92\n8.05\n8.02\nN/A\n8.09\n\n\n\n\n5 rows √ó 26 columns\n\n\n\n\n\nReviewing and Cleaning the Dataframe\nInspecting dataframe via df.describe(), df.info() and df.head(). There are a few things to clean up:\n\nRemove leading and trailing spaces\nReplace all ‚ÄòN/A‚Äô with NaN\nDrop columns with all NaN values\nconvert the ‚ÄòDate‚Äô column to datetime format\nconvert the rest of the columns to float\n\n\n\nCode\n# remove all blank spaces in the dataframe, except headers\nclean_df = df.iloc[:,:].map(lambda x:x.strip())\n# converting to lower case, if cell equals to 'n/a', then replace with empty string\nclean_df = clean_df.map(lambda x:np.nan if (x.lower() == 'n/a' or x =='') else x)\n# dropping all columns with all NaN values\nclean_df.dropna(axis=1, how='all', inplace=True)\n# change Date column to datetime and the rest to float\nclean_df[clean_df.columns[0]] = pd.to_datetime(clean_df[clean_df.columns[0]])\nfor col in clean_df.columns[1:]:\n    clean_df[col] = clean_df[col].astype(str).astype(float)\nclean_df.to_csv('clean_US_Treasury_Rates.csv', index=False)\nclean_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8960 entries, 0 to 8959\nData columns (total 15 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   Date    8960 non-null   datetime64[ns]\n 1   1 Mo    6060 non-null   float64       \n 2   1.5 Mo  172 non-null    float64       \n 3   2 Mo    1755 non-null   float64       \n 4   3 Mo    8956 non-null   float64       \n 5   4 Mo    753 non-null    float64       \n 6   6 Mo    8959 non-null   float64       \n 7   1 Yr    8959 non-null   float64       \n 8   2 Yr    8959 non-null   float64       \n 9   3 Yr    8959 non-null   float64       \n 10  5 Yr    8959 non-null   float64       \n 11  7 Yr    8959 non-null   float64       \n 12  10 Yr   8959 non-null   float64       \n 13  20 Yr   8020 non-null   float64       \n 14  30 Yr   7965 non-null   float64       \ndtypes: datetime64[ns](1), float64(14)\nmemory usage: 1.0 MB\n\n\n\nAll done! Ready for analysis."
  },
  {
    "objectID": "webScraper.html#initial-observations",
    "href": "webScraper.html#initial-observations",
    "title": "US Treasury Bond Web Scrapper",
    "section": "Initial Observations",
    "text": "Initial Observations\n\nI choose you, Seaborn and Matplotlib!\nUtilizing seaborn and matplotlib, we can visualize the trends and distribution in the U.S. Treasury Par Yield Curve Rates over time.\n\n\nData Distribution\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Melt df to long format\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\n\nf, ax = plt.subplots(figsize=(7, 6))\n# Plot the orbital period with horizontal boxes, vlag - blue/pink diverging color palette\nsns.boxplot(\n    data, x=\"Value\", y=\"Series\", hue=\"Series\", palette=\"vlag\"\n)\n\nax.set_xlabel(\"US Treasury Rates (%)\")\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\n\n\n\n\n\n\n\n\n\nBox plots allows us to view the distribution of rates for each maturity period. We can also observe that the short term rates has generally a lower rate than the long term rates, which is expected.\n1.5Mo and 4Mo maturity rate have significantly smaller range of data. Upon review, it appears that the 1.5Mo and 4Mo maturity rates were only added to the dataset in recent years, which has less data points compared to other rates."
  },
  {
    "objectID": "invertedYieldCurve.html",
    "href": "invertedYieldCurve.html",
    "title": "US Treasury Par Inverted Yield Curve Analysis",
    "section": "",
    "text": "Libraries needed:\n\npandas\nseaborn\nmatplotlib\nplotly"
  },
  {
    "objectID": "invertedYieldCurve.html#inverted-yield-curve",
    "href": "invertedYieldCurve.html#inverted-yield-curve",
    "title": "US Treasury Par Inverted Yield Curve Analysis",
    "section": "Inverted Yield Curve",
    "text": "Inverted Yield Curve\n‚ÄúWhen short-term rates are higher than long-term rates.‚Äù\nEconomists use the yield curve, which compares the interest rates difference bweteen ‚Äú10-year and 3-month‚Äù or ‚Äú10-year and 2-year‚Äù treasury rate, to determine if short-term investments are more profitable than long-term ones. When the yield curve inverts and the difference drops below zero, it serves as a warning sign of an impending recession.\n\n\nCode\n#calculate the inversed rate\nclean_df['10Yr_3Mo'] = clean_df['10 Yr']-clean_df['3 Mo']\nclean_df['10Yr_2Yr'] = clean_df['10 Yr']-clean_df['2 Yr']\n\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata = data[data['Series'].isin(['10Yr_3Mo', '10Yr_2Yr'])]\ndata.dropna(inplace=True)\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='flare')\nplt.title(\"10yr vs 2 Years/ 3 Months' Maturity Rate\")\nplt.ylabel(\"Inversed Rate (%)\")\nplt.legend(loc=\"upper right\")\nplt.axhline(y=0, color='grey',linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe assumption of recession is supported by the last 30 years of invasion occurrences. Recession happened in early 2000 (dot-com bubble), 2007-2009 (housing bubble), and 2020 (pendemic). The 3 recessions mentioned experienced a decrease below 0.\nUnusual Trend Observed between 2022-2024 with significant inverted rates. Let‚Äôs take a closer look using interactive visualization via Plotly.\n\n\nCode\nimport plotly.express as px\n\nfig = px.line(\n    data,\n    x='Date',\n    y='Value',\n    color='Series',\n    title=\"10yr vs 2 Years/3 Months' Maturity Rate\",\n    color_discrete_sequence=px.colors.sequential.Rainbow_r\n)\n\nfig.add_hline(y=0, line_dash='dash', line_color='gray')\n\n# Improve layout and interactivity\nfig.update_layout(\n    width=1000,\n    height=700,\n    yaxis_title=\"Inversed Rate (%)\",\n    # legend_title=\"Series\",\n    hovermode=\"x unified\",\n    template=\"plotly_white\",\n)\n\n# Add range slider and zoom buttons for date axis\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n\nfig.update_xaxes(range=[data['Date'].max() - pd.DateOffset(years=5), data['Date'].max()])\nfig.show()\n\n\n                            \n                                            \n\n\nHuge inversions happened between 2022-2024, with the spread dropping well below zero throughout the years. This trend heightened concerns about future economic growth. The spread is trending upwards and above zero starting in early 2025, indicating a potential economic recovery.\n\nConclusion\nThe U.S. Treasury Par Yield Curve Rates have shown a general downward trend over the past few decades, with notable volatility in short-term rates. The occurrence of inverted yield curves has historically been a precursor to recessions, highlighting the importance of monitoring these rates for economic forecasting. As of October 2025, we are seeing yield curve returning to normal after the recent inversion. Nonetheless, this has continued raised concerns over potential recession risks."
  },
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "US Treasury Par Yield Curve Rates PCA",
    "section": "",
    "text": "This analysis is performed using Python in a Jupyter Notebook environment. Ensure you have the following libraries installed:\n\npandas\nnumpy\nseaborn\nmatplotlib\nsklearn\n\n\n\nData was scraped from US Treasury‚Äôs Resource Center using BeautifulSoup and Requests, see previous analysis for full ETL code."
  },
  {
    "objectID": "PCA.html#lets-get-started",
    "href": "PCA.html#lets-get-started",
    "title": "US Treasury Par Yield Curve Rates PCA",
    "section": "",
    "text": "This analysis is performed using Python in a Jupyter Notebook environment. Ensure you have the following libraries installed:\n\npandas\nnumpy\nseaborn\nmatplotlib\nsklearn\n\n\n\nData was scraped from US Treasury‚Äôs Resource Center using BeautifulSoup and Requests, see previous analysis for full ETL code."
  },
  {
    "objectID": "PCA.html#principal-component-analysis-pca",
    "href": "PCA.html#principal-component-analysis-pca",
    "title": "US Treasury Par Yield Curve Rates PCA",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nSklearn, Here we go!\n\n\nStandardize df and Covariance Matrix Heatmap\nPCA allows us to reduce the dimensionality of our dataset while retaining most of the variance. This is particularly useful when dealing with datasets that have many correlated variables, as it helps to identify the underlying structure and patterns in the data.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler \nimport numpy.linalg as LA\n\nclean_df = pd.read_csv('clean_US_Treasury_Rates.csv')\nclean_df['Date'] = pd.to_datetime(clean_df['Date'])\n\ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\ncolnames=list(data.columns)\n# standardize df\nsc=StandardScaler()\ndata1=sc.fit_transform(data)\ndata1=pd.DataFrame(data1)\n\ndata1.columns = colnames\n\n# covariance matrix\ncov=pd.DataFrame.cov(data1)\n\nplt.figure(figsize=(12,8))\nsns.heatmap(cov, cmap=\"Spectral\", center=0, annot=True, fmt=\".2f\")\n\nplt.title(\"Covariance Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\nViewing the covariance matrix, we can see that the short-term rates (1Mo, 3Mo, 6Mo, 1Yr) are highly correlated with each other, as are the long-term rates (10Yr, 20Yr, 30Yr). However, there is a lower correlation between short-term and long-term rates, which is expected given their different sensitivities to economic conditions.\n\n\nCode\neval, evec = LA.eig(cov)\nfor i in range(3):\n    print('The PC{} is {}, its variance is {}, % variance is {}%, % cumulative variance is {}% '.format(\n        i+1\n        , np.round(evec[:, i],2)\n        , round(eval[i],2)\n        , round(eval[i]/eval.sum()*100,2)\n        , round(eval[:i+1].sum()/eval.sum()*100,2)))\n    \ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\n# bar graph\nPCs = [f'PC{i+1}' for i in range(11)]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nEigenValues = eval\nax.bar(PCs,EigenValues)\nplt.show()    \n\n\nThe PC1 is [0.3  0.3  0.31 0.31 0.31 0.32 0.32 0.31 0.3  0.27 0.26], its variance is 9.61, % variance is 87.37%, % cumulative variance is 87.37% \nThe PC2 is [-0.28 -0.29 -0.28 -0.26 -0.2  -0.12  0.06  0.19  0.33  0.46  0.52], its variance is 1.25, % variance is 11.33%, % cumulative variance is 98.71% \nThe PC3 is [ 0.44  0.33  0.19  0.   -0.25 -0.38 -0.41 -0.31 -0.07  0.19  0.38], its variance is 0.12, % variance is 1.06%, % cumulative variance is 99.77% \n\n\n\n\n\n\n\n\n\nYou can see that PC1 and PC2 explain most of the variance in the data (98.7% combined). The majority of the information in the original dataset can be captured in just two dimensions, which is a significant reduction from the original 11 dimensions.\nPC1 has coefficients that are all positive and relatively similar in magnitude (~0.3). It represents a general trend across all maturity rates. In contrast, First 6 of the PC2 coefficients are negative, and the last 5 are positive. This suggests PC2 captures the contrast between short-term and long-term rates.\n\n\nPCA Biplot\nTo further analyze PC1 and PC2, we will review are 2 main components in the biplot: arrows and points. The arrows represent the original variables (maturities), while the points represent the observations (dates).\n\n\nCode\nPCs = np.dot(data1, evec[:, :2])  # matrix multiplication: 11 variables (maturities) * weights (PC1, PC2)\n\ndata1 = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata1.dropna(inplace=True)\ndata1 = data1[['Date']]\ndata1['Year'] = data1['Date'].dt.year\ndata1.reset_index(drop=True, inplace=True)\npc_df = pd.DataFrame(PCs, columns=['PC1', 'PC2'])\npc_df = pd.concat([data1, pc_df], axis = 1)\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data = pc_df, x='PC1', y='PC2', alpha=0.7, hue = 'Year', palette='coolwarm')\n\n# Add arrows for variable loadings\nfor i, var in enumerate(data.columns):\n    #arrow start from 0,0, expand at the direction of PC1 and 2 for each maturity, *5 to make the arrows visible \n    plt.arrow(0, 0, evec[i,0]*5, evec[i,1]*5, color='r', alpha=0.7, head_width=0.05)\n    plt.text(evec[i,0]*5.2, evec[i,1]*5.2, var, color='r', ha='center', va='center')\n\n# 87.3% data explained by PC1 and 11.4% explained by PC2\nplt.xlabel(\"PC1 (%.1f%%)\" % (eval[0]/eval.sum()*100))\nplt.ylabel(\"PC2 (%.1f%%)\" % (eval[1]/eval.sum()*100))\n# horizontal and vertical lines at 0, y and x\nplt.axhline(0, color='grey', linewidth=1)\nplt.axvline(0, color='grey', linewidth=1)\nplt.title(\"PCA Biplot\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoints and Clusters\nEach point is weighted value of PC1 and PC2 for each observation (date). The position of the points indicates how the observations relate to the PC. The points are colored by year with a gradient from blue (earlier years) to red (later years). Each year has a distinct cluster, and the clusters allow us to observe how the observations change over time in relation to the PC.\n\n\nArrows (Loadings)\nThe arrows (loadings) represents the original variables (maturities). The arrows are pinned at the origin (0,0). The direction represents how each variable contributes to the PC, while the length indicates the strength of the contribution.\n\nLength of Arrows\nThe length of the arrows provides insight into the importance of each variable in explaining the variance captured by the PC. 30Yr has the longest arrow, indicating that the strongest influence on the PC1. In contrast, 5Yr has a shorter arrow, suggesting that lesser contribution to the variance.\n\n\nAngles of Arrows\nThe arrows for 1Mo, 3Mo, 6Mo, and 1Yr are close together, indicating that these short-term rates are highly correlated, same as arrows of 10Yr, 20Yr, and 30Yr for long-term rates. The arrows for 1Mo and 30Yr point in nearly opposite directions, suggesting a negative correlation between short-term and long-term rates. The arrows for 10Yr and 3Yr are nearly perpendicular, which indicates that the variables are uncorrelated.\n\n\nDirection of Arrows\n\n20Yr and 30Yr have high positive loadings on PC1 as they are closer to top axis/ loadings of PC1. 1Mo and 3Mo have lower loadings on PC1 smaller impact on the overall level of interest rates. None of the maturities have high loadings on PC2, indicating that PC2 captures the contrast between short-term and long-term rates rather than being dominated by any single maturity."
  },
  {
    "objectID": "PCA.html#so-whats-the-takeaway",
    "href": "PCA.html#so-whats-the-takeaway",
    "title": "US Treasury Par Yield Curve Rates PCA",
    "section": "So, What‚Äôs the takeaway?",
    "text": "So, What‚Äôs the takeaway?\nWith covariance matrix heatmap and PCA analysis, we see the strong correlations among short-term rates and among long-term rates, as well as the distinct patterns in how these rates have evolved over time. The PCA biplot provides a clear visualization of that relationships, emphasizing the contrast between short-term and long-term rates and their respective contributions to the overall variance in the dataset. The main source of variation in the yield curve comes from changes in long-term interest rates. In other words, the overall movement in the data is largely driven by the behavior of 20Yr and 30Yr bonds. In other words, the long bonds set the tone: when they move, the whole curve follows."
  }
]