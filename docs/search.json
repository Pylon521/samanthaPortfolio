[
  {
    "objectID": "webScraper.html",
    "href": "webScraper.html",
    "title": "US Treasury Bond Web Scraper",
    "section": "",
    "text": "Web scrap data via BeautifulSoup\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure & Distribution"
  },
  {
    "objectID": "webScraper.html#preview",
    "href": "webScraper.html#preview",
    "title": "US Treasury Bond Web Scraper",
    "section": "",
    "text": "Web scrap data via BeautifulSoup\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure & Distribution"
  },
  {
    "objectID": "webScraper.html#lets-get-started",
    "href": "webScraper.html#lets-get-started",
    "title": "US Treasury Bond Web Scraper",
    "section": "Let’s Get Started!",
    "text": "Let’s Get Started!\nThis analysis is performed using Python in a Jupyter Notebook environment. Make sure you have the following libraries installed:\n\npandas\ndatetime\nnumpy\nrequests\nmatplotlib\nbeautifulsoup4\nseaborn\n\n\nReview URL\nRecently, I stumbled upon a government website that lists daily Treasury rates by year and maturity. I noticed that the data could be accessed simply by changing the last four digits of the URL, so I built a scraper to pull every year’s data from 1990 all the way to today.\nTry it yourself, change “xxxx” to the desired year:\nhttps://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=xxxx\n\n\nInspecting HTML\nUse the latest URL (current year) to inspect the HTML table structure: headings and rows. You’ll notice there are 26 headings. The heading text looks clean, so we can store them to generate a DataFrame later. Within the table rows, there’s quite a bit of blank space in each cell, which is represented as text ‘N/A’. We’ll need to clean that up shortly.\n\n\nCode\nfrom bs4 import BeautifulSoup as BS\nimport requests as req\nimport datetime\nimport pandas as pd\nimport numpy as np\n\nurls=[]\nurl = \"https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=xxxx\"\n\nstart_year = 1990\nend_year = datetime.date.today().year\n\nyears = [str(yr) for yr in range(start_year, end_year+1)]\n\nfor year in years:\n    urls.append(url.replace('xxxx', year))\n\n# inspect the last URL to get the table headings\nhtml_text = req.get(urls[-1]).text\nsoup=BS(html_text, 'html.parser')\n\nprint('Scrapping from website:', soup.title.text)\nprint(f'This script is to scrape US Treasury Rate data from {start_year} to {end_year}.')\n\n# table = soup.find_all('tr') \n\n# column headings\nheadings = []\nfor item in soup.find_all('th'):\n    headings.append(item.text)\nprint('Table headings &lt;th&gt;:', headings)\nprint('Number of headings:', len(headings))\n\n# data rows\nall_rows = []\nfor item in soup.find_all('td'):\n    all_rows.append(item.text)\nprint('Table data &lt;td&gt;:', all_rows[:26])\n\n\nScrapping from website: Resource Center | U.S. Department of the Treasury\nThis script is to scrape US Treasury Rate data from 1990 to 2025.\nTable headings &lt;th&gt;: ['Date', '20 YR', '30 YR', 'Extrapolation Factor', '6 WEEKS BANK DISCOUNT', 'COUPON EQUIVALENT', '8 WEEKS BANK DISCOUNT', 'COUPON EQUIVALENT', '17 WEEKS BANK DISCOUNT', 'COUPON EQUIVALENT', '52 WEEKS BANK DISCOUNT', ' COUPON EQUIVALENT', '1 Mo', '1.5 Mo', '2 Mo', '3 Mo', '4 Mo', '6 Mo', '1 Yr', '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']\nNumber of headings: 26\nTable data &lt;td&gt;: ['01/02/2025\\n', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', 'N/A          ', '4.45          ', 'N/A          ', '4.36          ', '4.36          ', '4.31          ', '4.25          ', '4.17          ', '4.25          ', '4.29          ', '4.38          ', '4.47          ', '4.57          ', '4.86          ', '4.79          ']\n\n\nAfter reviewing the table structure, we can iterate through all the URLs to extract the complete dataset. All data is stored in a dataframe and exported as a CSV file for further analysis. ## Scraper Block\n\n\nCode\n# now scrape all the table data/ td from all the URLs\nall_rows = []\nfor url in urls:\n    html_text = req.get(url).text\n    soup = BS(html_text, 'html.parser')\n    table_rows = soup.find_all('tr')               \n    for tr in table_rows[1:]:\n        td = tr.find_all('td')\n        row=[i.text.strip('\\n') for i in td]       #i is each td element, i.text is to find all text in that i, .strip to remove entries starts with \\n\n        all_rows.append(row)\ndf=pd.DataFrame(data=all_rows, columns=headings)\ndf.to_csv('raw_US_Treasury_Rates.csv', index=False)\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\n20 YR\n30 YR\nExtrapolation Factor\n6 WEEKS BANK DISCOUNT\nCOUPON EQUIVALENT\n8 WEEKS BANK DISCOUNT\nCOUPON EQUIVALENT\n17 WEEKS BANK DISCOUNT\nCOUPON EQUIVALENT\n...\n4 Mo\n6 Mo\n1 Yr\n2 Yr\n3 Yr\n5 Yr\n7 Yr\n10 Yr\n20 Yr\n30 Yr\n\n\n\n\n0\n01/02/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.89\n7.81\n7.87\n7.90\n7.87\n7.98\n7.94\nN/A\n8.00\n\n\n1\n01/03/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.94\n7.85\n7.94\n7.96\n7.92\n8.04\n7.99\nN/A\n8.04\n\n\n2\n01/04/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.90\n7.82\n7.92\n7.93\n7.91\n8.02\n7.98\nN/A\n8.04\n\n\n3\n01/05/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.85\n7.79\n7.90\n7.94\n7.92\n8.03\n7.99\nN/A\n8.06\n\n\n4\n01/08/1990\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n...\nN/A\n7.88\n7.81\n7.90\n7.95\n7.92\n8.05\n8.02\nN/A\n8.09\n\n\n\n\n5 rows × 26 columns"
  },
  {
    "objectID": "webScraper.html#reviewing-and-cleaning-the-dataframe",
    "href": "webScraper.html#reviewing-and-cleaning-the-dataframe",
    "title": "US Treasury Bond Web Scraper",
    "section": "Reviewing and Cleaning the Dataframe",
    "text": "Reviewing and Cleaning the Dataframe\nInspecting dataframe via df.describe(), df.info() and df.head(). There are a few things to clean up:\n\nRemove leading and trailing spaces\nReplace all ‘N/A’ with NaN\nDrop columns with all NaN values\nconvert the ‘Date’ column to datetime format\nconvert the rest of the columns to float\n\n\n\nCode\n# remove all blank spaces in the dataframe, except headers\nclean_df = df.iloc[:,:].map(lambda x:x.strip())\n# converting to lower case, if cell equals to 'n/a', then replace with empty string\nclean_df = clean_df.map(lambda x:np.nan if (x.lower() == 'n/a' or x =='') else x)\n# dropping all columns with all NaN values\nclean_df.dropna(axis=1, how='all', inplace=True)\n# change Date column to datetime and the rest to float\nclean_df[clean_df.columns[0]] = pd.to_datetime(clean_df[clean_df.columns[0]])\nfor col in clean_df.columns[1:]:\n    clean_df[col] = clean_df[col].astype(str).astype(float)\nclean_df.to_csv('clean_US_Treasury_Rates.csv', index=False)\nclean_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8972 entries, 0 to 8971\nData columns (total 15 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   Date    8972 non-null   datetime64[ns]\n 1   1 Mo    6072 non-null   float64       \n 2   1.5 Mo  184 non-null    float64       \n 3   2 Mo    1767 non-null   float64       \n 4   3 Mo    8968 non-null   float64       \n 5   4 Mo    765 non-null    float64       \n 6   6 Mo    8971 non-null   float64       \n 7   1 Yr    8971 non-null   float64       \n 8   2 Yr    8971 non-null   float64       \n 9   3 Yr    8971 non-null   float64       \n 10  5 Yr    8971 non-null   float64       \n 11  7 Yr    8971 non-null   float64       \n 12  10 Yr   8971 non-null   float64       \n 13  20 Yr   8032 non-null   float64       \n 14  30 Yr   7977 non-null   float64       \ndtypes: datetime64[ns](1), float64(14)\nmemory usage: 1.0 MB\n\n\nAll done! Ready for analysis!"
  },
  {
    "objectID": "webScraper.html#observations",
    "href": "webScraper.html#observations",
    "title": "US Treasury Bond Web Scraper",
    "section": "Observations",
    "text": "Observations\n\nI choose you, Seaborn and Matplotlib!\nUtilizing seaborn and matplotlib, we can visualize the trends and distribution in the U.S. Treasury Par Yield Curve Rates over time.\n\n\nData Distribution\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Melt df to long format\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\n\nf, ax = plt.subplots(figsize=(7, 6))\n# Plot the orbital period with horizontal boxes, vlag - blue/pink diverging color palette\nsns.boxplot(\n    data, x=\"Value\", y=\"Series\", hue=\"Series\", palette=\"vlag\"\n)\n\nax.set_xlabel(\"US Treasury Rates (%)\")\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\n\n\n\n\n\n\n\n\n\nBox plots allows us to view the distribution of rates for each maturity period. We can also observe that the short term rates has generally a lower rate than the long term rates, which is expected.\n1.5Mo and 4Mo maturity rate have significantly smaller range of data. Upon review, it appears that the 1.5Mo and 4Mo maturity rates were only added to the dataset in recent years, which has less data points compared to other rates."
  },
  {
    "objectID": "webScraper.html#whats-next",
    "href": "webScraper.html#whats-next",
    "title": "US Treasury Bond Web Scraper",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn the next analysis, we will explore the dataset in depth and understand what an ‘inverted yield curve’ is. (US Treasury Bond Inverted Yield Curve)"
  },
  {
    "objectID": "invertedYieldCurve.html",
    "href": "invertedYieldCurve.html",
    "title": "US Treasury Par Inverted Yield Curve Analysis",
    "section": "",
    "text": "Prereq!\n\nETL Web Scraped Data from US Treasury\nData was scraped from US Treasury’s Resource Center using BeautifulSoup and Requests, see previous analysis for full ETL code.\n\n\nLibraries needed\n\npandas\nseaborn\nmatplotlib\nplotly\n\n\n\n\nTrends Over Time\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nclean_df = pd.read_csv('clean_US_Treasury_Rates.csv')\nclean_df['Date'] = pd.to_datetime(clean_df['Date'])\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\n\nsns.set_theme(style=\"whitegrid\")\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='crest')\nplt.title(\"Trend Overview\")\nplt.ylabel(\"US Treasury Rates (%)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe line chart shows the trends of different maturity rates from 1990 to 2025 are generally trending down. Unexpected upward spikes starting around 2021, likely due to economic recovery post-pandemic.\nMoreover, the short-term rates (light blue lines) are more volatile, while the long-term rates (dark blue lines) are more stable. There are a few periods that the short-term rates spike above the long-term rates, indicating an inverted yield curve.\n\n\nInverted Yield Curve\n“When short-term rates are higher than long-term rates.”\nEconomists use the yield curve, which compares the interest rates difference bweteen “10-year and 3-month” or “10-year and 2-year” treasury rate, to determine if short-term investments are more profitable than long-term ones. When the yield curve inverts and the difference drops below zero, it serves as a warning sign of an impending recession.\n\n\nCode\n#calculate the inversed rate\nclean_df['10Yr_3Mo'] = clean_df['10 Yr']-clean_df['3 Mo']\nclean_df['10Yr_2Yr'] = clean_df['10 Yr']-clean_df['2 Yr']\n\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata = data[data['Series'].isin(['10Yr_3Mo', '10Yr_2Yr'])]\ndata.dropna(inplace=True)\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='flare')\nplt.title(\"10yr vs 2 Years/ 3 Months' Maturity Rate\")\nplt.ylabel(\"Inversed Rate (%)\")\nplt.legend(loc=\"upper right\")\nplt.axhline(y=0, color='grey',linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe assumption of recession is supported by the last 30 years of invasion occurrences. Recession happened in early 2000 (dot-com bubble), 2007-2009 (housing bubble), and 2020 (pendemic). The 3 recessions mentioned experienced a decrease below 0.\n\nInteractive Viz via Plotly\nUnusual Trend Observed between 2022-2024 with significant inverted rates. Let’s take a closer look using interactive visualization via Plotly.\n\n\nCode\nimport plotly.express as px\n\nfig = px.line(\n    data,\n    x='Date',\n    y='Value',\n    color='Series',\n    title=\"10yr vs 2 Years/3 Months' Maturity Rate\",\n    color_discrete_sequence=px.colors.sequential.Rainbow_r\n)\n\nfig.add_hline(y=0, line_dash='dash', line_color='gray')\n\n# Improve layout and interactivity\nfig.update_layout(\n    # width=1000,\n    # height=700,\n    yaxis_title=\"Inversed Rate (%)\",\n    # legend_title=\"Series\",\n    hovermode=\"x unified\",\n    template=\"plotly_white\",\n)\n\n# Add range slider and zoom buttons for date axis\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n\nfig.update_xaxes(range=[data['Date'].max() - pd.DateOffset(years=5), data['Date'].max()])\nfig.show()\n\n\n                            \n                                            \n\n\nHuge inversions happened between 2022-2024, with the spread dropping well below zero throughout the years. This trend heightened concerns about future economic growth. The spread is trending upwards and above zero starting in early 2025, indicating a potential economic recovery.\n\n\n\nConclusion\nThe U.S. Treasury Par Yield Curve Rates have shown a general downward trend over the past few decades, with notable volatility in short-term rates. The occurrence of inverted yield curves has historically been a precursor to recessions, highlighting the importance of monitoring these rates for economic forecasting. As of October 2025, we are seeing yield curve returning to normal after the recent inversion. Nonetheless, this has continued raised concerns over potential recession risks.\n\n\nWhat’s Next?\nSee the next analysis to understand what Principal Component Analysis (PCA) is and which maturity drives the overall trend. (US Treasury Bond PCA)"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Latest Projects",
    "section": "",
    "text": "US Treasury Bond Web Scraper\n\n\nPublished: Oct 2025\n\n\nWeb scraping US Treasury Rate site with step by step ETL.\n\n\n\n \n\n\nInverted Yield Curve Analysis\n\n\nPublished: Oct 2025\n\n\nAnalyzing historical inverted yield curve data to explore its correlation with past economic recessions and predict future trends.\n\n\n\n \n\n\nTreasury Yields PCA\n\n\nPublished: Oct 2025\n\n\nApplying Principal Component Analysis (PCA) to treasury yields to identify primary drivers of yield curve movements, such as level, slope, and curvature."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Samantha Pang",
    "section": "",
    "text": "Data Science / Plant Lover / Climber\n“The flow state, often called “being in the zone,” is a mental state where you are fully absorbed in an activity,and losing track of time.”\nGlad you are here! This website is to showcase Samantha’s skills and passions and ultimately to inspire you to pursue what you love, aka being in the flow!\n\n✉️ Contact: theflow365@emailhub.kr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam P",
    "section": "",
    "text": "Samantha’s Latest Projects:\n\n \n\n\nTreasury Yields PCA\n\n\nPublished: Oct 2025\n\n\nApplying Principal Component Analysis (PCA) to treasury yields to identify primary drivers of yield curve movements, such as level, slope, and curvature.\n\n\n\n \n\n\nInverted Yield Curve Analysis\n\n\nPublished: Oct 2025\n\n\nAnalyzing historical inverted yield curve data to explore its correlation with past economic recessions and predict future trends.\n\n\n\n \n\n\nUS Treasury Bond Web Scraper\n\n\nPublished: Oct 2025\n\n\nWeb scraping US Treasury Rate site with step by step ETL."
  },
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "US Treasury Par Yield Curve Rates Principal Component Analysis (PCA)",
    "section": "",
    "text": "Data was scraped from US Treasury’s Resource Center using BeautifulSoup and Requests, see previous analysis for full ETL code.\n\n\n\n\npandas\nnumpy\nseaborn\nmatplotlib\nsklearn"
  },
  {
    "objectID": "PCA.html#prereq",
    "href": "PCA.html#prereq",
    "title": "US Treasury Par Yield Curve Rates Principal Component Analysis (PCA)",
    "section": "",
    "text": "Data was scraped from US Treasury’s Resource Center using BeautifulSoup and Requests, see previous analysis for full ETL code.\n\n\n\n\npandas\nnumpy\nseaborn\nmatplotlib\nsklearn"
  },
  {
    "objectID": "PCA.html#standardize-df-and-covariance-matrix-heatmap",
    "href": "PCA.html#standardize-df-and-covariance-matrix-heatmap",
    "title": "US Treasury Par Yield Curve Rates Principal Component Analysis (PCA)",
    "section": "Standardize df and Covariance Matrix Heatmap",
    "text": "Standardize df and Covariance Matrix Heatmap\n\nSklearn, Here we go!\nPCA allows us to reduce the dimensionality of our dataset while retaining most of the variance. This is particularly useful when dealing with datasets that have many correlated variables, as it helps to identify the underlying structure and patterns in the data.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler \nimport numpy.linalg as LA\n\nclean_df = pd.read_csv('clean_US_Treasury_Rates.csv')\nclean_df['Date'] = pd.to_datetime(clean_df['Date'])\n\ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\ncolnames=list(data.columns)\n# standardize df\nsc=StandardScaler()\ndata1=sc.fit_transform(data)\ndata1=pd.DataFrame(data1)\n\ndata1.columns = colnames\n\n# covariance matrix\ncov=pd.DataFrame.cov(data1)\n\nplt.figure(figsize=(12,8))\nsns.heatmap(cov, cmap=\"Spectral\", center=0, annot=True, fmt=\".2f\")\n\nplt.title(\"Covariance Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\nViewing the covariance matrix, we can see that the short-term rates (1Mo, 3Mo, 6Mo, 1Yr) are highly correlated with each other, as are the long-term rates (10Yr, 20Yr, 30Yr). However, there is a lower correlation between short-term and long-term rates, which is expected given their different sensitivities to economic conditions.\n\n\nCode\neval, evec = LA.eig(cov)\nfor i in range(3):\n    print('The PC{} is {}, its variance is {}, % variance is {}%, % cumulative variance is {}% '.format(\n        i+1\n        , np.round(evec[:, i],2)\n        , round(eval[i],2)\n        , round(eval[i]/eval.sum()*100,2)\n        , round(eval[:i+1].sum()/eval.sum()*100,2)))\n    \ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\n# bar graph\nPCs = [f'PC{i+1}' for i in range(11)]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nEigenValues = eval\nax.bar(PCs,EigenValues)\nplt.show()    \n\n\nThe PC1 is [0.3  0.3  0.3  0.31 0.31 0.32 0.32 0.31 0.3  0.27 0.26], its variance is 9.62, % variance is 87.4%, % cumulative variance is 87.4% \nThe PC2 is [-0.28 -0.29 -0.28 -0.26 -0.2  -0.12  0.06  0.19  0.33  0.46  0.52], its variance is 1.24, % variance is 11.31%, % cumulative variance is 98.71% \nThe PC3 is [ 0.44  0.33  0.18  0.   -0.25 -0.38 -0.41 -0.31 -0.07  0.19  0.38], its variance is 0.12, % variance is 1.06%, % cumulative variance is 99.77% \n\n\n\n\n\n\n\n\n\nYou can see that PC1 and PC2 explain most of the variance in the data (98.7% combined). The majority of the information in the original dataset can be captured in just two dimensions, which is a significant reduction from the original 11 dimensions.\nPC1 has coefficients that are all positive and relatively similar in magnitude (~0.3). It represents a general trend across all maturity rates. In contrast, First 6 of the PC2 coefficients are negative, and the last 5 are positive. This suggests PC2 captures the contrast between short-term and long-term rates."
  },
  {
    "objectID": "PCA.html#pca-biplot",
    "href": "PCA.html#pca-biplot",
    "title": "US Treasury Par Yield Curve Rates Principal Component Analysis (PCA)",
    "section": "PCA Biplot",
    "text": "PCA Biplot\nTo further analyze PC1 and PC2, we will review are 2 main components in the biplot: arrows and points. The arrows represent the original variables (maturities), while the points represent the observations (dates).\n\n\nCode\nPCs = np.dot(data1, evec[:, :2])  # matrix multiplication: 11 variables (maturities) * weights (PC1, PC2)\n\ndata1 = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata1.dropna(inplace=True)\ndata1 = data1[['Date']]\ndata1['Year'] = data1['Date'].dt.year\ndata1.reset_index(drop=True, inplace=True)\npc_df = pd.DataFrame(PCs, columns=['PC1', 'PC2'])\npc_df = pd.concat([data1, pc_df], axis = 1)\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data = pc_df, x='PC1', y='PC2', alpha=0.7, hue = 'Year', palette='coolwarm')\n\n# Add arrows for variable loadings\nfor i, var in enumerate(data.columns):\n    #arrow start from 0,0, expand at the direction of PC1 and 2 for each maturity, *5 to make the arrows visible \n    plt.arrow(0, 0, evec[i,0]*5, evec[i,1]*5, color='r', alpha=0.7, head_width=0.05)\n    plt.text(evec[i,0]*5.2, evec[i,1]*5.2, var, color='r', ha='center', va='center')\n\n# 87.3% data explained by PC1 and 11.4% explained by PC2\nplt.xlabel(\"PC1 (%.1f%%)\" % (eval[0]/eval.sum()*100))\nplt.ylabel(\"PC2 (%.1f%%)\" % (eval[1]/eval.sum()*100))\n# horizontal and vertical lines at 0, y and x\nplt.axhline(0, color='grey', linewidth=1)\nplt.axvline(0, color='grey', linewidth=1)\nplt.title(\"PCA Biplot\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoints and Clusters\nEach point is weighted value of PC1 and PC2 for each observation (date). The position of the points indicates how the observations relate to the PC. The points are colored by year with a gradient from blue (earlier years) to red (later years). Each year has a distinct cluster, and the clusters allow us to observe how the observations change over time in relation to the PC.\n\n\nArrows (Loadings)\nThe arrows (loadings) represents the original variables (maturities). The arrows are pinned at the origin (0,0). The direction represents how each variable contributes to the PC, while the length indicates the strength of the contribution.\n\n\nLength of Arrows\nThe length of the arrows provides insight into the importance of each variable in explaining the variance captured by the PC. 30Yr has the longest arrow, indicating that the strongest influence on the PC1. In contrast, 5Yr has a shorter arrow, suggesting that lesser contribution to the variance.\n\n\nAngles of Arrows\nThe arrows for 1Mo, 3Mo, 6Mo, and 1Yr are close together, indicating that these short-term rates are highly correlated, same as arrows of 10Yr, 20Yr, and 30Yr for long-term rates. The arrows for 1Mo and 30Yr point in nearly opposite directions, suggesting a negative correlation between short-term and long-term rates. The arrows for 10Yr and 3Yr are nearly perpendicular, which indicates that the variables are uncorrelated.\n\n\nDirection of Arrows\n\n20Yr and 30Yr have high positive loadings on PC1 as they are closer to top axis/ loadings of PC1. 1Mo and 3Mo have lower loadings on PC1 smaller impact on the overall level of interest rates. None of the maturities have high loadings on PC2, indicating that PC2 captures the contrast between short-term and long-term rates rather than being dominated by any single maturity."
  },
  {
    "objectID": "PCA.html#whats-the-takeaway",
    "href": "PCA.html#whats-the-takeaway",
    "title": "US Treasury Par Yield Curve Rates Principal Component Analysis (PCA)",
    "section": "What’s the takeaway?",
    "text": "What’s the takeaway?\nWith covariance matrix heatmap and PCA analysis, we see the strong correlations among short-term rates and among long-term rates, as well as the distinct patterns in how these rates have evolved over time. The PCA biplot provides a clear visualization of that relationships, emphasizing the contrast between short-term and long-term rates and their respective contributions to the overall variance in the dataset. The main source of variation in the yield curve comes from changes in long-term interest rates. In other words, the overall movement in the data is largely driven by the behavior of 20Yr and 30Yr bonds. In other words, the long bonds set the tone: when they move, the whole curve follows."
  }
]