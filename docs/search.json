[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi",
    "section": "",
    "text": "I am Samantha.\n\nCoder/ Analyst/ Plant Lover\n\nGlad you are here! This page is to showcase my skills and passions. Ultimately, I hope to inspire you to pursue what you love, aka being in the flow!\n“The flow state, often called “being in the zone,” is a mental state where you are fully absorbed in an activity, and losing track of time.”\n\n\n\n\nSee my latest completed project:\n\n\n\n\n\n\n\nUS Treasury Bond Web Scraper/ PCA"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "",
    "text": "collecting data via beautiful soul (web scraper)\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure / Distribution\nLearn about PCA and yield dynamics"
  },
  {
    "objectID": "analysis.html#lets-get-started",
    "href": "analysis.html#lets-get-started",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Let’s get started!",
    "text": "Let’s get started!\nThis analysis is performed using Python in a Jupyter Notebook environment. Ensure you have the following libraries installed:\n\npandas\ndatetime\nnumpy\nrequests\nmatplotlib\nbeautifulsoup4\nseaborn\nmatplotlib\nsklearn\n\n\nExtracting the Data via Web Scraping\nData is scrapped from the U.S. Department of the Treasury. Date range: 1990 to Present\nUse the first URL to inspect the table structure, and get the table headings, then loop through all the URLs to get the table data.\n\n\nCode\nfrom bs4 import BeautifulSoup as BS\nimport requests as req\nimport datetime\nimport pandas as pd\nimport numpy as np\n\nurls=[]\nurl = \"https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=xxxx\"\n\nstart_year = 1990\nend_year = datetime.date.today().year\n\nyears = [str(yr) for yr in range(start_year, end_year+1)]\n\nfor year in years:\n    urls.append(url.replace('xxxx', year))\n\n# inspect the first URL to get the table headings\nhtml_text = req.get(urls[0]).text\nsoup=BS(html_text, 'html.parser')\n\nprint('Scrapping from website:', soup.title.text)\nprint(f'This script is to scrap US Treasury Rate data from {start_year} to {end_year}.')\n\n# column headings\nheadings = []\n# table = soup.find('table', class_ = 'usa-table views-table views-view-table cols-26') \n# table_rows = table.find_all('tr')\n\nfor item in soup.find_all('th'):\n    headings.append(item.text)\n# print('Table headings &lt;th&gt;:', headings)\n\n# now scrap all the table data/ td from all the URLs\nall_rows = []\nfor url in urls:\n    html_text = req.get(url).text\n    soup = BS(html_text, 'html.parser')\n    table_rows = soup.find_all('tr')               \n    for tr in table_rows[1:]:\n        td = tr.find_all('td')\n        row=[i.text.strip('\\n') for i in td]       #i is each td element, i.text is to find all text in that i, .strip to remove entries starts with \\n\n        all_rows.append(row)\ndf=pd.DataFrame(data=all_rows, columns=headings)\n\n# df.to_csv('raw_US_Treasury_Rates.csv', index=False)\n\n\nScrapping from website: Resource Center | U.S. Department of the Treasury\nThis script is to scrap US Treasury Rate data from 1990 to 2025.\n\n\n\n\nReviewing and Cleaning the Data\nInspecting dataframe via df.describe(), df.info() and df.head(). There are a few things to clean up:\n\nRemove leading and trailing spaces\nReplace all ‘N/A’ with NaN\nDrop columns with all NaN values\nconvert the ‘Date’ column to datetime format\nconvert the rest of the columns to float\n\n\nAll done! Ready for analysis.\n\n\nCode\n# display(df.describe(), df.head())\n# noticed there are space in N/A cell, \n# print(\"N/A cell: '\", df['20 YR'][0], \"'\")\n# remove all blank spaces in the dataframe, except headers\nclean_df = df.iloc[:,:].map(lambda x:x.strip())\n# converting to lower case, if cell equals to 'n/a', then replace with empty string\nclean_df = clean_df.map(lambda x:np.nan if (x.lower() == 'n/a' or x =='') else x)\n# dropping all columns with all NaN values\nclean_df.dropna(axis=1, how='all', inplace=True)\n# change Date column to datetime and the rest to float\nclean_df[clean_df.columns[0]] = pd.to_datetime(clean_df[clean_df.columns[0]])\nfor col in clean_df.columns[1:]:\n    clean_df[col] = clean_df[col].astype(str).astype(float)\n# clean_df.info()\n# clean_df.head()"
  },
  {
    "objectID": "analysis.html#initial-observations",
    "href": "analysis.html#initial-observations",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Initial observations",
    "text": "Initial observations\n\nI choose you, Seaborn and Matplotlib!\n\n\nData Distribution\nUtilizing seaborn and matplotlib, we will visualize the trends and distribution in the U.S. Treasury Par Yield Curve Rates over time.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Melt df to long format\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\n\nf, ax = plt.subplots(figsize=(7, 6))\n# Plot the orbital period with horizontal boxes, vlag - blue/pink diverging color palette\nsns.boxplot(\n    data, x=\"Value\", y=\"Series\", hue=\"Series\", palette=\"vlag\"\n)\n\nax.set_xlabel(\"US Treasury Rates (%)\")\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\n\n\n\n\n\n\n\n\n\nBox plots allows us to view the distribution of rates for each maturity period. We can also observe that the short term rates has generally a lower rate than the long term rates, which is expected.\n1.5Mo and 4Mo maturity rate have significantly smaller range of data. Upon review, it appears that the 1.5Mo and 4Mo maturity rates were only added to the dataset in recent years, which has less data points compared to other rates.\n\n\nTrends Over Time\n\n\nCode\nsns.set_theme(style=\"whitegrid\")\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='crest')\nplt.title(\"Data Overview\")\nplt.ylabel(\"US Treasury Rates (%)\")\n# plt.\nplt.show()\n\n\n\n\n\n\n\n\n\nThe line chart below shows the trends of different maturity rates from 1990 to 2024 are generally trending down. Unexpected upward spikes starting around 2021, likely due to economic recovery post-pandemic.\nMoreover, the short-term rates (1 month, 3 month, 6 month, 1 year) are more volatile, while the long-term rates (10 year, 20 year, 30 year) are more stable. There are a few period that the short-term rates spike above the long-term rates, indicating an inverted yield curve, which we will explore this further."
  },
  {
    "objectID": "analysis.html#inverted-yield-curve",
    "href": "analysis.html#inverted-yield-curve",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Inverted Yield Curve",
    "text": "Inverted Yield Curve\n“When short-term rates are higher than long-term rates.”\nEconomists use the yield curve, which compares the interest rates difference bweteen “10-year and 3-month” or “10-year and 2-year” treasury rate, to determine if short-term investments are more profitable than long-term ones. When the yield curve inverts and the difference drops below zero, it serves as a warning sign of an impending recession.\n\n\nCode\n#calculate the inversed rate\nclean_df['10Yr_3Mo'] = clean_df['10 Yr']-clean_df['3 Mo']\nclean_df['10Yr_2Yr'] = clean_df['10 Yr']-clean_df['2 Yr']\n\ndata = clean_df.melt(id_vars='Date', var_name='Series', value_name='Value')\ndata.dropna(inplace=True)\ndata = data[data['Series'].isin(['10Yr_3Mo', '10Yr_2Yr'])]\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data, x='Date', y='Value', hue='Series', palette='flare')\nplt.title(\"10yr vs 2 Years/ 3 Months' Maturity Rate\")\nplt.ylabel(\"Inversed Rate (%)\")\nplt.legend(loc=\"upper right\")\nplt.axhline(y=0, color='grey',linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe assumption of recession is supported by the last 30 years of invasion occurrences. Recession happened in early 2000 (dot-com bubble), 2007-2009 (housing bubble), and 2020 (pendemic). The 3 recessions mentioned experienced a decrease below 0, as shown in the chart above."
  },
  {
    "objectID": "analysis.html#principal-component-analysis-pca",
    "href": "analysis.html#principal-component-analysis-pca",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nSklearn, Here we go!\n\n\nStandardize df and Covariance Matrix Heatmap\nPCA allows us to reduce the dimensionality of our dataset while retaining most of the variance. This is particularly useful when dealing with datasets that have many correlated variables, as it helps to identify the underlying structure and patterns in the data.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler \nimport numpy.linalg as LA\n\ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\ncolnames=list(data.columns)\n# standardize df\nsc=StandardScaler()\ndata1=sc.fit_transform(data)\ndata1=pd.DataFrame(data1)\n\ndata1.columns = colnames\n\n# covariance matrix\ncov=pd.DataFrame.cov(data1)\n\nplt.figure(figsize=(12,8))\nsns.heatmap(cov, cmap=\"Spectral\", center=0, annot=True, fmt=\".2f\")\n\nplt.title(\"Covariance Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\nViewing the covariance matrix, we can see that the short-term rates (1Mo, 3Mo, 6Mo, 1Yr) are highly correlated with each other, as are the long-term rates (10Yr, 20Yr, 30Yr). However, there is a lower correlation between short-term and long-term rates, which is expected given their different sensitivities to economic conditions.\n\n\nCode\neval, evec = LA.eig(cov)\nfor i in range(3):\n    print('The PC{} is {}, its variance is {}, % variance is {}%, % cumulative variance is {}% '.format(\n        i+1\n        , np.round(evec[:, i],2)\n        , round(eval[i],2)\n        , round(eval[i]/eval.sum()*100,2)\n        , round(eval[:i+1].sum()/eval.sum()*100,2)))\n    \ndata = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata.dropna(inplace=True)\ndata = data.set_index('Date')\n# bar graph\nPCs = [f'PC{i+1}' for i in range(11)]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nEigenValues = eval\nax.bar(PCs,EigenValues)\nplt.show()    \n\n\nThe PC1 is [0.3  0.3  0.31 0.31 0.31 0.32 0.32 0.31 0.3  0.27 0.26], its variance is 9.61, % variance is 87.36%, % cumulative variance is 87.36% \nThe PC2 is [-0.28 -0.29 -0.28 -0.26 -0.2  -0.12  0.06  0.19  0.33  0.46  0.52], its variance is 1.25, % variance is 11.35%, % cumulative variance is 98.71% \nThe PC3 is [ 0.44  0.33  0.19  0.   -0.25 -0.38 -0.41 -0.31 -0.07  0.19  0.38], its variance is 0.12, % variance is 1.06%, % cumulative variance is 99.77% \n\n\n\n\n\n\n\n\n\nYou can see that PC1 and PC2 explain most of the variance in the data (98.7% combined). The majority of the information in the original dataset can be captured in just two dimensions, which is a significant reduction from the original 11 dimensions.\nPC1 has coefficients that are all positive and relatively similar in magnitude (~0.3). It represents a general trend across all maturity rates. In contrast, First 6 of the PC2 coefficients are negative, and the last 5 are positive. This suggests PC2 captures the contrast between short-term and long-term rates.\n\n\nPCA Biplot\nTo further analyze PC1 and PC2, we will review are 2 main components in the biplot: arrows and points. The arrows represent the original variables (maturities), while the points represent the observations (dates).\n\n\nCode\nPCs = np.dot(data1, evec[:, :2])  # matrix multiplication: 11 variables (maturities) * weights (PC1, PC2)\n\ndata1 = clean_df[['Date', '1 Mo', '3 Mo', '6 Mo', '1 Yr',\n                 '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']].copy()\ndata1.dropna(inplace=True)\ndata1 = data1[['Date']]\ndata1['Year'] = data1['Date'].dt.year\ndata1.reset_index(drop=True, inplace=True)\npc_df = pd.DataFrame(PCs, columns=['PC1', 'PC2'])\npc_df = pd.concat([data1, pc_df], axis = 1)\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data = pc_df, x='PC1', y='PC2', alpha=0.7, hue = 'Year', palette='coolwarm')\n\n# Add arrows for variable loadings\nfor i, var in enumerate(data.columns):\n    #arrow start from 0,0, expand at the direction of PC1 and 2 for each maturity, *5 to make the arrows visible \n    plt.arrow(0, 0, evec[i,0]*5, evec[i,1]*5, color='r', alpha=0.7, head_width=0.05)\n    plt.text(evec[i,0]*5.2, evec[i,1]*5.2, var, color='r', ha='center', va='center')\n\n# 87.3% data explained by PC1 and 11.4% explained by PC2\nplt.xlabel(\"PC1 (%.1f%%)\" % (eval[0]/eval.sum()*100))\nplt.ylabel(\"PC2 (%.1f%%)\" % (eval[1]/eval.sum()*100))\n# horizontal and vertical lines at 0, y and x\nplt.axhline(0, color='grey', linewidth=1)\nplt.axvline(0, color='grey', linewidth=1)\nplt.title(\"PCA Biplot\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoints and Clusters\nEach point is weighted value of PC1 and PC2 for each observation (date). The position of the points indicates how the observations relate to the PC. The points are colored by year with a gradient from blue (earlier years) to red (later years). Each year has a distinct cluster, and the clusters allow us to observe how the observations change over time in relation to the PC.\n\n\nArrows (Loadings)\nThe arrows (loadings) represents the original variables (maturities). The arrows are pinned at the origin (0,0). The direction represents how each variable contributes to the PC, while the length indicates the strength of the contribution.\n\nLength of Arrows\nThe length of the arrows provides insight into the importance of each variable in explaining the variance captured by the PC. 30Yr has the longest arrow, indicating that the strongest influence on the PC1. In contrast, 5Yr has a shorter arrow, suggesting that lesser contribution to the variance.\n\n\nAngles of Arrows\nThe arrows for 1Mo, 3Mo, 6Mo, and 1Yr are close together, indicating that these short-term rates are highly correlated, same as arrows of 10Yr, 20Yr, and 30Yr for long-term rates. The arrows for 1Mo and 30Yr point in nearly opposite directions, suggesting a negative correlation between short-term and long-term rates. The arrows for 10Yr and 3Yr are nearly perpendicular, which indicates that the variables are uncorrelated.\n\n\nDirection of Arrows\n\n20Yr and 30Yr have high positive loadings on PC1 as they are closer to top axis/ loadings of PC1. 1Mo and 3Mo have lower loadings on PC1 smaller impact on the overall level of interest rates. None of the maturities have high loadings on PC2, indicating that PC2 captures the contrast between short-term and long-term rates rather than being dominated by any single maturity."
  },
  {
    "objectID": "analysis.html#so-whats-the-takeaway",
    "href": "analysis.html#so-whats-the-takeaway",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "So, What’s the takeaway?",
    "text": "So, What’s the takeaway?\nThe U.S. Treasury Par Yield Curve Rates have shown a general downward trend over the past few decades, with notable volatility in short-term rates. The occurrence of inverted yield curves has historically been a precursor to recessions, highlighting the importance of monitoring these rates for economic forecasting. As of October 2025, we are seeing yield curve returning to normal after the recent inversion in 2024, indicating potential economic recovery.\nWith covariance matrix heatmap and PCA analysis, we can see the strong correlations among short-term rates and among long-term rates, as well as the distinct patterns in how these rates have evolved over time. The PCA biplot provides a clear visualization of that relationships, emphasizing the contrast between short-term and long-term rates and their respective contributions to the overall variance in the dataset. The main source of variation in the yield curve comes from changes in long-term interest rates. In other words, the overall movement in the data is largely driven by the behavior of 20Yr and 30Yr bonds. In other words, the long bonds set the tone: when they move, the whole curve follows."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Let’s Connect",
    "section": "",
    "text": "You can find me here:\n\n\n💼 LinkedIn\n🧑‍💻 GitHub \n✉️ Contact: theflow365@emailhub.kr"
  },
  {
    "objectID": "analysis.html#preview",
    "href": "analysis.html#preview",
    "title": "US Treasury Par Yield Curve Rates Analysis",
    "section": "",
    "text": "collecting data via beautiful soul (web scraper)\nData source: U.S. Department of the Treasury\nInitial ETL\nReview Data Structure / Distribution\nLearn about PCA and yield dynamics"
  }
]